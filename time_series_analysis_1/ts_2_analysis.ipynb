{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Forecasting with Bayesian Inference\n",
    "\n",
    "This notebook demonstrates the use of Bayesian inference for sales forecasting using various probabilistic programming techniques. We will use the `numpyro` library to define and fit our models, and `plotnine` for visualization.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Bayesian inference allows us to incorporate prior knowledge and quantify uncertainty in our predictions. This notebook will guide you through the process of building a Bayesian model for sales forecasting, fitting the model using Markov Chain Monte Carlo (MCMC) and Stochastic Variational Inference (SVI), and visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries And Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set XLA_FLAGS before JAX is imported\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, labs, theme_minimal, theme_bw, scale_x_continuous, scale_x_discrete, scale_x_datetime\n",
    "import patsy\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "#from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from numpyro.infer import MCMC, NUTS, MCMC, NUTS #, SVI, Trace_ELBO\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model\n",
    "\n",
    "In this section, we define the Bayesian model used for sales forecasting. The model incorporates various components such as random walk for the latent state, day-of-the-week effects, day-of-the-year effects, and price elasticity. The model is implemented using the `numpyro` library, which allows for efficient and scalable Bayesian inference.\n",
    "\n",
    "### 1.1 Auxiliary Functions\n",
    "\n",
    "These auxiliary functions are essential for data preprocessing and transformation:\n",
    "\n",
    "- `periodic_rbf`: Computes a periodic Gaussian radial basis function (RBF).\n",
    "- `compute_doy_basis`: Computes 12 periodic Gaussian basis functions for seasonal effects.\n",
    "- `read_data`: Reads and preprocesses the sales data from a CSV file.\n",
    "- `init_values`: Initializes values for the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a periodic Gaussian radial basis function (RBF)\n",
    "def periodic_rbf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Computes a periodic Gaussian radial basis function (RBF).\n",
    "    \n",
    "    Args:\n",
    "        x: Scaled day-of-year values (range [0,1]).\n",
    "        mu: Center of the Gaussian basis function.\n",
    "        sigma: Controls the spread of the Gaussian.\n",
    "    \n",
    "    Returns:\n",
    "        RBF values preserving periodicity.\n",
    "    \"\"\"\n",
    "    # compute cyclic distance to mu\n",
    "    periodic_distance = jnp.minimum(jnp.abs(x - mu), 1 - jnp.abs(x - mu))\n",
    "    # compute RBF value\n",
    "    return jnp.exp(- (periodic_distance ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "def compute_doy_basis(yday_fraction, sigma = 30/365.25, n_centers = 12):\n",
    "    \"\"\"\n",
    "    Computes 12 periodic Gaussian basis functions for seasonal effects.\n",
    "    \n",
    "    Args:\n",
    "        yday_fraction: Normalized day of the year (range [0,1]).\n",
    "        yday_factor: Scaling factor for basis function width.\n",
    "    \n",
    "    Returns:\n",
    "        A JAX array with 12 columns representing the 12 monthly basis functions.\n",
    "    \"\"\"\n",
    "    # Define centers of Gaussian basis functions\n",
    "    month_centers = jnp.linspace( 1/(2*n_centers), 1-1/(2*n_centers), n_centers)\n",
    "    \n",
    "    # Generate an array of shape (length of input, 12) with the RBF values\n",
    "    doy_basis = jnp.stack([periodic_rbf(yday_fraction, mu, sigma) for mu in month_centers], axis=-1)\n",
    "\n",
    "    # Subtract each row's mean to enforce sum-to-zero constraint\n",
    "    doy_basis_centered = doy_basis - jnp.mean(doy_basis, axis=-1, keepdims=True)\n",
    "    \n",
    "    return doy_basis_centered\n",
    "\n",
    "def read_data(fname):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses the sales data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        fname: The filename of the CSV file containing the sales data.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with the following keys:\n",
    "            - sales: An array of sales data.\n",
    "            - log_price: An array of log-transformed prices.\n",
    "            - wday: An array of day-of-the-week values.\n",
    "            - yday_fraction: An array of normalized day-of-the-year values.\n",
    "    \"\"\"\n",
    "    # Read the CSV file using polars\n",
    "    df = pl.read_csv(fname)\n",
    "    \n",
    "    # Convert the 'date' column to date type\n",
    "    df = df.with_columns(pl.col(\"date\").str.to_date())\n",
    "\n",
    "    # Extract sales, and log price data as a numpy arrays\n",
    "    sales = df[\"sales\"].to_numpy()\n",
    "    log_price = df[\"log_price\"].to_numpy()\n",
    "    \n",
    "    # Extract day-of-the-week values\n",
    "    wday = df[\"date\"].dt.weekday().to_numpy()\n",
    "    \n",
    "    # Extract day-of-the-year values\n",
    "    yday = df[\"date\"].dt.ordinal_day().to_numpy()\n",
    "    \n",
    "    # Determine if the year is a leap year\n",
    "    is_leap_year = df[\"date\"].dt.is_leap_year().to_numpy()\n",
    "    \n",
    "    # Normalize day-of-the-year values\n",
    "    yday_fraction = yday / (365 + is_leap_year)\n",
    "    \n",
    "    # Return the preprocessed data as a dictionary\n",
    "    return {\n",
    "        \"sales\": sales,\n",
    "        \"log_price\": log_price,\n",
    "        \"wday\": wday,\n",
    "        \"yday_fraction\": yday_fraction\n",
    "    }\n",
    "\n",
    "#def init_values(sales: jnp.array, log_price_centered: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1):\n",
    "#    \"\"\"\n",
    "#    \"\"\"\n",
    "#    # to-do: implement downsampling_factor\n",
    "#    log_state_est = jnp.log(sales)\n",
    "#    log_state_mean_est = jnp.mean(log_state_est)\n",
    "#    log_state_delta_est = jnp.diff(log_state_est)\n",
    "#    log_state_delta_sd_est = jnp.std(log_state_delta_est)\n",
    "#\n",
    "#    return {\n",
    "#        \"log_sigma\": jnp.log( log_state_delta_sd_est ),\n",
    "#        \"log_state_mean\": log_state_mean_est,\n",
    "#        \"log_state_delta\": log_state_delta_est,\n",
    "#        \"wday_coefficients\": jnp.array([0.0]*6),\n",
    "#        \"yday_coefficients\": jnp.array([0.0]*12),\n",
    "#        \"log_elasticity\": jnp.array([0.0])\n",
    "#    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model\n",
    "\n",
    "#### 1.2.1 Key Features\n",
    "\n",
    "- Sales are modeled using a **stochastic Poisson process**, where the expected rate $\\lambda_t$ evolves over time.\n",
    "- The **latent sales rate** follows a random walk, allowing it to drift nonstationarily.\n",
    "- **Seasonal components** (day-of-the-week and annual patterns) adjust for structured demand variations.\n",
    "- **Price elasticity** is explicitly modeled, ensuring sensitivity to pricing dynamics.\n",
    "- The model is implemented in numpyro, enabling scalable Bayesian inference.\n",
    "\n",
    "\n",
    "#### 1.2.1 Model Overview\n",
    "\n",
    "We model the sales time series as a **stochastic process** where the underlying rate of sales evolves over time. This evolution follows a **random walk structure**, but with systematic adjustments for covariates such as price, day-of-the-week effects, and day-of-the-year effects. The rate of sales $\\lambda_t$ on day $t$ is a function of captures *(i)* systematic covariate effects ($z_t$), *(ii)*\n",
    "a global baseline ($\\mu_\\tau$), and *(iii)* the latent dynamic component ($\\tau_t$).\n",
    "\n",
    "$$\n",
    "log~\\lambda_t = z_t + \\mu_\\tau + \\tau_t\n",
    "$$\n",
    "\n",
    "##### 1.2.1.1 Latent States Dynamics\n",
    "\n",
    "The baseline sales level $\\tau_t$ follows a **random walk**. Because all contrast matrices for structured effects are centered, $\\mu_\\tau + \\tau_t$ can be interpreted as the average latent sales rate on $\\tau_t$. \n",
    "\n",
    "$$\n",
    "\\tau_t = \\tau_{t-1} + \\delta_t, \\quad \\delta_t \\sim \\mathcal{N}(0, \\sigma_\\tau)\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\mu_\\tau \\sim \\text{Exponential}(1), \\quad \\sigma_\\tau \\sim \\mathcal{N}(1)\n",
    "$$\n",
    "\n",
    "\n",
    "##### 1.2.1.2 Structured Effects\n",
    "\n",
    "We further accounted for systematic effects of *(i)* day of the week, *(ii)* day of the year, and *(iii)* price.\n",
    "\n",
    "- For day of the week effects, we used a contrast matrix $\\mathbf{C}_{\\text{wday}}$ with sliding differences.\n",
    "- For day of the year effects, we used a matrix of Gaussian radial basis functions $\\mathbf{B}_{\\text{yday}}$.\n",
    "- Price elasticity is modelled using a centered log price \n",
    "\n",
    "Similarly, the day-of-the-year effects are modeled using a seasonality basis matrix $\\mathbf{B}_{\\text{yday}}$, which represents periodic seasonal patterns using Gaussian radial basis functions (RBFs).\n",
    "\n",
    "\n",
    "- **Day-of-the-week effects**:\n",
    "\n",
    "$$\n",
    "  zw_t = \\mathbf{C}_{\\text{wday}} \\cdot \\beta_{\\text{wday}}, \\quad \\beta_{\\text{wday}} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "- **Day-of-the-year effects**:\n",
    "\n",
    "$$\n",
    "  zy_t = \\mathbf{B}_{\\text{yday}} \\cdot \\beta_{\\text{yday}}, \\quad \\beta_{\\text{yday}} \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "- **Price elasticity**:\n",
    "\n",
    "$$\n",
    "  ze_t = \\text{log\\_price\\_centered} \\cdot e, \\quad \\log(-e) \\sim \\mathcal{N^{+}}(0, 1)\n",
    "$$\n",
    "\n",
    "- **Sum of structural effects**:\n",
    "\n",
    "$$\n",
    "  z_t = zw_t + zy_t + ze_t\n",
    "$$\n",
    "\n",
    "\n",
    "##### 1.2.1.3 Emissions Model\n",
    "\n",
    "\n",
    "The final latent state combines all effects:\n",
    "\n",
    "$$\n",
    "\\text{state}_t = \\exp(\\tau_t + w_t + y_t + e_t)\n",
    "$$\n",
    "\n",
    "\n",
    "Observed sales follow a **Poisson distribution**, ensuring discrete, non-negative observations:\n",
    "\n",
    "$$\n",
    "X_t \\sim \\text{Poisson}(\\lambda_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\lambda_t = \\text{state}_t\n",
    "$$\n",
    "\n",
    "\n",
    "This formulation balances flexibility (random walk) with structured demand influences (seasonality, price sensitivity), making it suitable for forecasting applications.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "The latent component $\\tau_t$ follows a **random walk**:\n",
    "\n",
    "$$\n",
    "\\tau_t = \\tau_{t-1} + \\delta_t\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "ùúè\n",
    "ùë°\n",
    "œÑ \n",
    "t\n",
    "‚Äã\n",
    "  follows a random walk:\n",
    "\n",
    "ùúè\n",
    "ùë°\n",
    "=\n",
    "ùúè\n",
    "ùë°\n",
    "‚àí\n",
    "1\n",
    "+\n",
    "ùõø\n",
    "ùë°\n",
    "œÑ \n",
    "t\n",
    "‚Äã\n",
    " =œÑ \n",
    "t‚àí1\n",
    "‚Äã\n",
    " +Œ¥ \n",
    "t\n",
    "‚Äã\n",
    " \n",
    "where the step size \n",
    "ùõø\n",
    "ùë°\n",
    "Œ¥ \n",
    "t\n",
    "‚Äã\n",
    "  is drawn from a normal distribution:\n",
    "\n",
    "ùõø\n",
    "ùë°\n",
    "‚àº\n",
    "ùëÅ\n",
    "(\n",
    "0\n",
    ",\n",
    "ùúé\n",
    "ùúè\n",
    ")\n",
    "Œ¥ \n",
    "t\n",
    "‚Äã\n",
    " ‚àºN(0,œÉ \n",
    "œÑ\n",
    "‚Äã\n",
    " )\n",
    "This structure allows \n",
    "ùúè\n",
    "ùë°\n",
    "œÑ \n",
    "t\n",
    "‚Äã\n",
    "  to drift over time, making it suitable for capturing long-term shifts in sales trends (e.g., gradual growth or decline).\n",
    "\n",
    "We also define priors for the key parameters:\n",
    "\n",
    "The global baseline level follows an exponential prior:\n",
    "ùúá\n",
    "ùúè\n",
    "‚àº\n",
    "Exponential\n",
    "(\n",
    "1\n",
    ")\n",
    "Œº \n",
    "œÑ\n",
    "‚Äã\n",
    " ‚àºExponential(1)\n",
    "This ensures that \n",
    "ùúá\n",
    "ùúè\n",
    "Œº \n",
    "œÑ\n",
    "‚Äã\n",
    "  is non-negative and prevents extreme values.\n",
    "The random walk step size has a normal prior:\n",
    "ùúé\n",
    "ùúè\n",
    "‚àº\n",
    "ùëÅ\n",
    "(\n",
    "1\n",
    ")\n",
    "œÉ \n",
    "œÑ\n",
    "‚Äã\n",
    " ‚àºN(1)\n",
    "This controls the magnitude of daily fluctuations in \n",
    "ùúè\n",
    "ùë°\n",
    "œÑ \n",
    "t\n",
    "‚Äã\n",
    " .\n",
    "\n",
    "\n",
    "\n",
    "The sales time series will be modeled as a random walk model with covariate-dependent drift, where $\\lambda_t$ is the poisson rate on day $t$, $\\tau_t$ is <todo>, and $z_t$ is the effect of covariates.\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta_t \\sim \\mathcal{N}(0, \\sigma_\\tau) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_\\tau \\sim \\text{Exponential}(1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_\\tau \\sim \\mathcal{N}(1)\n",
    "$$\n",
    "\n",
    "The model is defined using the `numpyro` library, which allows for efficient and scalable Bayesian inference. At its core, it\n",
    "incorporates various components such as random walk for the latent state, day-of-the-week effects, day-of-the-year effects, and price elasticity. The model is implemented as follows:\n",
    "\n",
    "\n",
    "1. **Random Walk for Latent State**:\n",
    "    \\[\n",
    "    \\log(\\sigma) \\sim \\text{Gumbel}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\sigma = \\exp(\\log(\\sigma))\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_mean}) \\sim \\text{Normal}(0, 5)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_delta}) \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_base}) = \\text{state\\_mean} + \\sigma \\cdot \\text{state\\_delta}\n",
    "    \\]\n",
    "\n",
    "2. **Day-of-the-Week Effects**:\n",
    "    \\[\n",
    "    \\text{wday\\_coefficients} \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{wday\\_effect} = \\text{wday\\_coefficients} \\cdot \\text{wday}\n",
    "    \\]\n",
    "\n",
    "3. **Day-of-the-Year Effects**:\n",
    "    \\[\n",
    "    \\text{yday\\_coefficients} \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{yday\\_effect} = \\text{yday\\_coefficients} \\cdot \\text{yday\\_fraction}\n",
    "    \\]\n",
    "\n",
    "4. **Price Elasticity**:\n",
    "    \\[\n",
    "    \\log(\\text{elasticity}) \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{elasticity} = -\\exp(\\log(\\text{elasticity}))\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{price\\_effect} = \\text{log\\_price\\_centered} \\cdot \\text{elasticity}\n",
    "    \\]\n",
    "\n",
    "5. **State Computation**:\n",
    "    \\[\n",
    "    \\text{state} = \\exp(\\log(\\text{state\\_base}) + \\text{price\\_effect} + \\text{yday\\_effect} + \\text{wday\\_effect})\n",
    "    \\]\n",
    "\n",
    "6. **Poisson Emissions**:\n",
    "    \\[\n",
    "    \\text{sales} \\sim \\text{Poisson}(\\text{state})\n",
    "    \\]\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma \\sim \\text{Exponential}(1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{\\tau} \\sim \\text{Normal}(0, 5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(\\tau) \\sim \\text{Normal}(\\mu_{\\tau}, \\sigma_{\\tau})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{\\tau=1}^{T} \\Delta \\tau\n",
    "$$\n",
    "\n",
    "The model can be described mathematically as follows:\n",
    "\n",
    "1. **Random Walk for Latent State**:\n",
    "    \\[\n",
    "    \\log(\\sigma) \\sim \\text{Gumbel}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\sigma = \\exp(\\log(\\sigma))\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_mean}) \\sim \\text{Normal}(0, 5)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_delta}) \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\log(\\text{state\\_base}) = \\text{state\\_mean} + \\sigma \\cdot \\text{state\\_delta}\n",
    "    \\]\n",
    "\n",
    "2. **Day-of-the-Week Effects**:\n",
    "    \\[\n",
    "    \\text{wday\\_coefficients} \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{wday\\_effect} = \\text{wday\\_coefficients} \\cdot \\text{wday}\n",
    "    \\]\n",
    "\n",
    "3. **Day-of-the-Year Effects**:\n",
    "    \\[\n",
    "    \\text{yday\\_coefficients} \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{yday\\_effect} = \\text{yday\\_coefficients} \\cdot \\text{yday\\_fraction}\n",
    "    \\]\n",
    "\n",
    "4. **Price Elasticity**:\n",
    "    \\[\n",
    "    \\log(\\text{elasticity}) \\sim \\text{Normal}(0, 1)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{elasticity} = -\\exp(\\log(\\text{elasticity}))\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{price\\_effect} = \\text{log\\_price\\_centered} \\cdot \\text{elasticity}\n",
    "    \\]\n",
    "\n",
    "5. **State Computation**:\n",
    "    \\[\n",
    "    \\text{state} = \\exp(\\log(\\text{state\\_base}) + \\text{price\\_effect} + \\text{yday\\_effect} + \\text{wday\\_effect})\n",
    "    \\]\n",
    "\n",
    "6. **Poisson Emissions**:\n",
    "    \\[\n",
    "    \\text{sales} \\sim \\text{Poisson}(\\text{state})\n",
    "    \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_local_level_poisson(sales: jnp.array, log_price_centered: jnp.array, wday, yday_fraction: jnp.array, \n",
    "                              contrasts_sdif_t: jnp.array, contrasts_wday: jnp.array, contrasts_yday: jnp.array, \n",
    "                              downsampling_factor = 1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    n_obs = len(sales)\n",
    "    n_states = contrasts_sdif_t.shape[0]\n",
    " \n",
    "    def sample_random_walk(contrasts_sdif_t, n_states):\n",
    "        log_sigma = numpyro.sample(\"log_sigma\", dist.Gumbel(0, 1))\n",
    "        sigma = numpyro.deterministic(\"sigma\", jnp.exp(log_sigma))\n",
    "        log_state_mean = numpyro.sample(\"log_state_mean\", dist.Normal(0, 5))\n",
    "        log_state_delta = numpyro.sample( \"log_state_delta\", dist.Normal(0, 1), sample_shape=(n_states-1,))\n",
    "        log_state_base = numpyro.deterministic(\"log_state_base\", jnp.dot(contrasts_sdif_t, log_state_delta) * sigma + log_state_mean )\n",
    "        return log_state_base\n",
    "\n",
    "    def sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states):\n",
    "        log_state_base_downsampled = sample_random_walk(contrasts_sdif_t, n_states)\n",
    "        \n",
    "        idx_n_weight = jnp.array(range(0, n_obs))/downsampling_factor\n",
    "        idx_1 = jnp.array( jnp.floor(idx_n_weight), dtype=int)\n",
    "        idx_2 = jnp.array( jnp.ceil(idx_n_weight), dtype=int)\n",
    "        weight_2 = idx_n_weight - idx_1\n",
    "\n",
    "        state_before = log_state_base_downsampled[idx_1]\n",
    "        state_after = log_state_base_downsampled[idx_2]\n",
    "        return (1-weight_2)*state_before + weight_2*state_after\n",
    "        \n",
    "    def sample_wday_effect(contrasts_wday, wday):\n",
    "        # Prior for day-of-the-week effects (6 coefficients)\n",
    "        wday_coefficients = numpyro.sample(\"wday_coefficients\", dist.Normal(0, 1), sample_shape=(6,))\n",
    "\n",
    "        # Compute wday effect per observation (sum-to-zero constraint applied via contrasts)\n",
    "        wday_effects = jnp.dot(contrasts_wday, wday_coefficients)\n",
    "        return jnp.array([wday_effects[d - 1] for d in wday]) # to-do: just use an index vector instead of a loop\n",
    "\n",
    "    def sample_yday_effect(contrasts_yday, yday_fraction):\n",
    "        # Prior for yearly seasonality effects (12 coefficients)\n",
    "        yday_coefficients = numpyro.sample(\"yday_coefficients\", dist.Normal(0, 1), sample_shape=(12,))\n",
    "        return jnp.dot(contrasts_yday, yday_coefficients)\n",
    "\n",
    "    def sample_price_effect(log_price_centered):\n",
    "        # Prior for price elasticity\n",
    "        log_elasticity = numpyro.sample( \"log_elasticity\", dist.HalfNormal(0, 1.5) )\n",
    "        elasticity = numpyro.deterministic( \"elasticity\", -1 * jnp.exp( log_elasticity ))\n",
    "        return log_price_centered * elasticity\n",
    "\n",
    "\n",
    "    # Sample random walk    \n",
    "    if n_obs == n_states:\n",
    "        log_state_base = sample_random_walk(contrasts_sdif_t, n_states)\n",
    "    else:\n",
    "        log_state_base = sample_downsampled_random_walk(contrasts_sdif_t, n_obs, n_states)\n",
    "\n",
    "    # Sample day-of-the-week effects\n",
    "    wday_effect = sample_wday_effect(contrasts_wday, wday)\n",
    "\n",
    "    # Sample day-of-the-year effects\n",
    "    yday_effect = sample_yday_effect(contrasts_yday, yday_fraction)\n",
    "\n",
    "    # Sample elasticity effect\n",
    "    price_effect = sample_price_effect(log_price_centered)\n",
    "\n",
    "    # Compute state\n",
    "    state = numpyro.deterministic(\"state\", jnp.exp(log_state_base + price_effect + yday_effect + wday_effect))\n",
    "\n",
    "    # Compute log-likelihood for poisson emissions\n",
    "    numpyro.sample(\"sales\", dist.Poisson(rate=state), obs=sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Fitting Logic\n",
    "\n",
    "- Functions to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_arguments(sales: jnp.array, log_price: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1):\n",
    "    \"\"\" \n",
    "    \"\"\"    \n",
    "    n_obs = len(sales)\n",
    "    if downsampling_factor == 1:\n",
    "        n_states = n_obs\n",
    "    else:\n",
    "        n_states = int( np.floor(n_obs/downsampling_factor) + 1 ) \n",
    "    \n",
    "    # Define contrast matrix for random walk (T coefficients, sum-to-zero constraint)\n",
    "    contrasts_sdif_t = patsy.contrasts.Diff().code_without_intercept(range(0, n_states)).matrix\n",
    "\n",
    "    # Define contrast matrix for day-of-the-week effects (6 coefficients, sum-to-zero constraint)\n",
    "    contrasts_wday = patsy.contrasts.Diff().code_without_intercept(range(0,7)).matrix  # 7 days ‚Üí 6 contrasts\n",
    "\n",
    "    # Compute yday effect per observation (sum-to-zero constraint applied via contrasts)\n",
    "    contrasts_yday = compute_doy_basis(yday_fraction, sigma = 30/365.25, n_centers = 12)\n",
    "\n",
    "    # Compute centered log price differences\n",
    "    log_price_centered = log_price - jnp.mean(log_price)\n",
    "\n",
    "    # Set up the model parameters\n",
    "    model_arguments = {'sales': sales, 'log_price_centered': log_price_centered, 'wday': wday, 'yday_fraction': yday_fraction,\n",
    "                       'downsampling_factor': downsampling_factor,\n",
    "                       'contrasts_sdif_t': contrasts_sdif_t, 'contrasts_wday': contrasts_wday, 'contrasts_yday': contrasts_yday}\n",
    "    \n",
    "    # Prepare init values for parameters \n",
    "    init_params = init_values(sales, log_price_centered, wday, yday_fraction)\n",
    "\n",
    "    return init_params, model_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nuts(sales: jnp.array, log_price: jnp.array, wday, yday_fraction: jnp.array, downsampling_factor = 1, n_chains = 1, num_warmup=1_000, num_samples=1_000):\n",
    "    \"\"\" Runs NUTS MCMC inference on the model \n",
    "    \"\"\"\n",
    "    rng_key = random.PRNGKey(0)\n",
    "    \n",
    "    n_obs = len(sales)\n",
    "    \n",
    "    # Prepare model arguments\n",
    "    init_params, model_arguments = prepare_model_arguments(sales = sales, log_price = log_price, wday = wday, yday_fraction = yday_fraction, downsampling_factor = downsampling_factor)\n",
    "\n",
    "    rng_key, rng_key_ = random.split(rng_key)\n",
    "\n",
    "    numpyro.set_host_device_count(n_chains)\n",
    "\n",
    "    reparam_model = model_local_level_poisson\n",
    "    kernel = NUTS(reparam_model, step_size=0.01, max_tree_depth=8)\n",
    "    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=n_chains)\n",
    "    mcmc.run( rng_key_, **model_arguments) #, init_params = init_params\n",
    "\n",
    "    return mcmc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = read_data(\"sales_synthetic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e7ed69f6d42b5860c44226992657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974e5f68fbac495a9fc0110ff560da0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808dfda243e54a8899bdbb51869ab290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1b2511c0844898abbccaba4f7edcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_fit = run_nuts(data['sales'], data['log_price'], data['wday'], data['yday_fraction'], downsampling_factor = 7, n_chains = 4, num_warmup=1_000, num_samples=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(m_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elasticity</th>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "sigma       0.069  0.007   0.057    0.083      0.000    0.000    1000.0   \n",
       "elasticity -0.550  0.265  -1.026   -0.100      0.006    0.004    1523.0   \n",
       "\n",
       "            ess_tail  r_hat  \n",
       "sigma         1964.0    1.0  \n",
       "elasticity    1478.0    1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import arviz as az\n",
    "#summary = az.summary(m['mcmc'])\n",
    "summary.loc[['sigma', 'elasticity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elasticity</th>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_elasticity</th>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-1.727</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_sigma</th>\n",
       "      <td>-2.677</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-2.868</td>\n",
       "      <td>-2.493</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_state_base[0]</th>\n",
       "      <td>2.246</td>\n",
       "      <td>0.119</td>\n",
       "      <td>2.033</td>\n",
       "      <td>2.473</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2231.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_state_base[1]</th>\n",
       "      <td>2.273</td>\n",
       "      <td>0.107</td>\n",
       "      <td>2.078</td>\n",
       "      <td>2.482</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yday_coefficients[7]</th>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>416.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yday_coefficients[8]</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>392.0</td>\n",
       "      <td>819.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yday_coefficients[9]</th>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.011</td>\n",
       "      <td>398.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yday_coefficients[10]</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>387.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yday_coefficients[11]</th>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.011</td>\n",
       "      <td>386.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
       "elasticity            -0.550  0.265  -1.026   -0.100      0.006    0.004   \n",
       "log_elasticity        -0.733  0.560  -1.727    0.235      0.015    0.014   \n",
       "log_sigma             -2.677  0.102  -2.868   -2.493      0.003    0.001   \n",
       "log_state_base[0]      2.246  0.119   2.033    2.473      0.003    0.002   \n",
       "log_state_base[1]      2.273  0.107   2.078    2.482      0.002    0.002   \n",
       "...                      ...    ...     ...      ...        ...      ...   \n",
       "yday_coefficients[7]  -0.115  0.379  -0.844    0.609      0.019    0.010   \n",
       "yday_coefficients[8]   0.167  0.374  -0.469    0.915      0.019    0.009   \n",
       "yday_coefficients[9]  -0.205  0.383  -0.951    0.502      0.019    0.011   \n",
       "yday_coefficients[10]  0.003  0.368  -0.681    0.693      0.019    0.009   \n",
       "yday_coefficients[11] -0.135  0.385  -0.885    0.564      0.020    0.011   \n",
       "\n",
       "                       ess_bulk  ess_tail  r_hat  \n",
       "elasticity               1523.0    1478.0   1.00  \n",
       "log_elasticity           1523.0    1478.0   1.00  \n",
       "log_sigma                1000.0    1964.0   1.00  \n",
       "log_state_base[0]        2231.0    2755.0   1.00  \n",
       "log_state_base[1]        1992.0    2649.0   1.00  \n",
       "...                         ...       ...    ...  \n",
       "yday_coefficients[7]      416.0     762.0   1.00  \n",
       "yday_coefficients[8]      392.0     819.0   1.01  \n",
       "yday_coefficients[9]      398.0     810.0   1.00  \n",
       "yday_coefficients[10]     387.0     759.0   1.01  \n",
       "yday_coefficients[11]     386.0     668.0   1.00  \n",
       "\n",
       "[1800 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(data=idata_parameters, var_names=[\"log_sigma\", \"log_price_coefficient\", \"yday_coefficients\", \"wday_coefficients\"], round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(m['mcmc'].print_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from datetime import date\n",
    "rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from datetime import date\n",
    "rng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import multivariate_normal\n",
    "\n",
    "import blackjax\n",
    "import blackjax.smc.resampling as resampling\n",
    "from blackjax.smc import extend_params\n",
    "\n",
    "from numpyro.infer.util import initialize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params, model_arguments = prepare_model_arguments(sales, log_price, wday, yday_fraction)    \n",
    "\n",
    "rng_key, init_key = jax.random.split(rng_key)\n",
    "init_params, potential_fn_gen, *_ = initialize_model(\n",
    "    init_key,\n",
    "    model_local_level_poisson,\n",
    "    model_kwargs=model_arguments,\n",
    "    dynamic_args=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdensity_fn = lambda position: -potential_fn_gen(**model_arguments)(position)\n",
    "initial_position = init_params.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax\n",
    "\n",
    "num_warmup = 2000\n",
    "\n",
    "adapt = blackjax.window_adaptation(\n",
    "    blackjax.nuts, logdensity_fn, target_acceptance_rate=0.8\n",
    ")\n",
    "rng_key, warmup_key = jax.random.split(rng_key)\n",
    "(last_state, parameters), _ = adapt.run(warmup_key, initial_position, num_warmup)\n",
    "kernel = blackjax.nuts(logdensity_fn, **parameters).step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, info = kernel(rng_key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states, (\n",
    "        infos.acceptance_rate,\n",
    "        infos.is_divergent,\n",
    "        infos.num_integration_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 1000\n",
    "rng_key, sample_key = jax.random.split(rng_key)\n",
    "states, infos = inference_loop(sample_key, kernel, last_state, num_sample)\n",
    "#_ = states.position[\"mu\"].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_result, posterior_parameters, posterior_generated = run_svi(sales, log_price, wday, yday_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_parameters = az.from_dict(\n",
    "     posterior = { k: np.expand_dims(a=np.asarray(v), axis=0) for k, v in posterior_parameters.items() },\n",
    " )\n",
    "idata_generated = az.from_dict(\n",
    "     posterior={ k: np.expand_dims(a=np.asarray(v), axis=0) for k, v in posterior_generated.items() },\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#az.summary(data=idata_generated, var_names=[\"sigma\"], round_to=3)\n",
    "az.summary(data=idata_parameters, var_names=[\"log_sigma\", \"log_price_coefficient\", \"yday_coefficients\", \"wday_coefficients\"], round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(data=idata_generated, var_names=[\"state\"], round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = jnp.mean(posterior_generated['state'], axis=0).tolist()\n",
    "df = df.with_columns([ pl.Series(\"state\", state) ])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({ 'date': df[\"date\"].to_numpy(), 'sales': df[\"sales\"].to_numpy(), 'state': m['state'] })\n",
    "ggplot(x, aes(x='date', y='sales')) + geom_point() + geom_line(aes(y='state'), color = \"red\") + theme_bw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
